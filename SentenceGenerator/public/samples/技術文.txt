1. はじめに

近年、自然言語処理分野では大規模言語モデル（LLM）の発展が著しい。
しかし、これらのモデルは高い計算コストと大量のパラメータを必要とするため、
限られた計算資源しか持たない環境での利用には課題が残る。
本研究では、軽量化を目的としたTransformerモデルを用い、
日本語ニュース記事の要約タスクにおける性能を評価する。

2. 実験方法

実験では、オープンデータとして公開されている日本語ニュースコーパス（約8万件）を利用した。
BERTベースおよびDistilBERTベースのモデルを比較対象とし、
Rougeスコアを指標として自動要約の品質を測定した。
さらに、パラメータ数の削減による処理時間短縮とメモリ使用量の変化も評価した。

3. 結果

解析の結果、DistilBERTモデルはBERTモデルの約40％のパラメータ数で動作し、
平均Rouge-1スコアは0.89と、高い要約精度を維持できることが確認された。
また、推論時間は約30％短縮され、GPUメモリ使用量も25％削減された。
特に短文要約（100文字以内）では、意味の一貫性が高く、
文法的誤りも少ない傾向が見られた。

4. 考察

軽量モデルは、大規模なサーバ環境を必要とせず、
組み込み機器やローカル端末での自然言語処理を可能にする点で有用である。
一方で、長文文脈の保持能力には限界があり、
今後は階層型アーキテクチャや知識蒸留手法の適用が課題となる。

5. 結論

本研究では、軽量Transformerモデルが日本語要約タスクにおいて
十分な性能を発揮することを示した。
今後は、生成タスクへの拡張や多言語対応の検証を通じ、
小規模モデルの実用性向上を目指す。
